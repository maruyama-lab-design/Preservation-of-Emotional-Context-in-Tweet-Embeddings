{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        1\n",
      "2        0\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "43195    1\n",
      "43196    2\n",
      "43197    2\n",
      "43198    0\n",
      "43199    2\n",
      "Name: Avg. Readers_Joy, Length: 43200, dtype: int64\n",
      "0        [0, 2, 0, 0, 0, 0, 0, 0]\n",
      "1        [1, 0, 0, 2, 0, 0, 0, 0]\n",
      "2        [0, 0, 0, 1, 0, 0, 0, 0]\n",
      "3        [0, 1, 0, 0, 0, 0, 1, 0]\n",
      "4        [1, 0, 0, 1, 0, 0, 0, 0]\n",
      "                   ...           \n",
      "43195    [1, 0, 0, 1, 0, 0, 0, 0]\n",
      "43196    [2, 0, 2, 0, 0, 0, 0, 0]\n",
      "43197    [2, 0, 0, 0, 0, 0, 0, 0]\n",
      "43198    [0, 0, 2, 0, 0, 0, 0, 0]\n",
      "43199    [2, 0, 0, 0, 0, 0, 0, 1]\n",
      "Name: readers_emotion_intensities, Length: 43200, dtype: object\n",
      "---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18237 entries, 0 to 18236\n",
      "Data columns (total 45 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   Sentence                     18237 non-null  object\n",
      " 1   UserID                       18237 non-null  int64 \n",
      " 2   Datetime                     18237 non-null  object\n",
      " 3   Train/Dev/Test               18237 non-null  object\n",
      " 4   Writer_Joy                   18237 non-null  int64 \n",
      " 5   Writer_Sadness               18237 non-null  int64 \n",
      " 6   Writer_Anticipation          18237 non-null  int64 \n",
      " 7   Writer_Surprise              18237 non-null  int64 \n",
      " 8   Writer_Anger                 18237 non-null  int64 \n",
      " 9   Writer_Fear                  18237 non-null  int64 \n",
      " 10  Writer_Disgust               18237 non-null  int64 \n",
      " 11  Writer_Trust                 18237 non-null  int64 \n",
      " 12  Reader1_Joy                  18237 non-null  int64 \n",
      " 13  Reader1_Sadness              18237 non-null  int64 \n",
      " 14  Reader1_Anticipation         18237 non-null  int64 \n",
      " 15  Reader1_Surprise             18237 non-null  int64 \n",
      " 16  Reader1_Anger                18237 non-null  int64 \n",
      " 17  Reader1_Fear                 18237 non-null  int64 \n",
      " 18  Reader1_Disgust              18237 non-null  int64 \n",
      " 19  Reader1_Trust                18237 non-null  int64 \n",
      " 20  Reader2_Joy                  18237 non-null  int64 \n",
      " 21  Reader2_Sadness              18237 non-null  int64 \n",
      " 22  Reader2_Anticipation         18237 non-null  int64 \n",
      " 23  Reader2_Surprise             18237 non-null  int64 \n",
      " 24  Reader2_Anger                18237 non-null  int64 \n",
      " 25  Reader2_Fear                 18237 non-null  int64 \n",
      " 26  Reader2_Disgust              18237 non-null  int64 \n",
      " 27  Reader2_Trust                18237 non-null  int64 \n",
      " 28  Reader3_Joy                  18237 non-null  int64 \n",
      " 29  Reader3_Sadness              18237 non-null  int64 \n",
      " 30  Reader3_Anticipation         18237 non-null  int64 \n",
      " 31  Reader3_Surprise             18237 non-null  int64 \n",
      " 32  Reader3_Anger                18237 non-null  int64 \n",
      " 33  Reader3_Fear                 18237 non-null  int64 \n",
      " 34  Reader3_Disgust              18237 non-null  int64 \n",
      " 35  Reader3_Trust                18237 non-null  int64 \n",
      " 36  Avg. Readers_Joy             18237 non-null  int64 \n",
      " 37  Avg. Readers_Sadness         18237 non-null  int64 \n",
      " 38  Avg. Readers_Anticipation    18237 non-null  int64 \n",
      " 39  Avg. Readers_Surprise        18237 non-null  int64 \n",
      " 40  Avg. Readers_Anger           18237 non-null  int64 \n",
      " 41  Avg. Readers_Fear            18237 non-null  int64 \n",
      " 42  Avg. Readers_Disgust         18237 non-null  int64 \n",
      " 43  Avg. Readers_Trust           18237 non-null  int64 \n",
      " 44  readers_emotion_intensities  18237 non-null  object\n",
      "dtypes: int64(41), object(4)\n",
      "memory usage: 6.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from lib import * # wrime data is loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_wrime = pd.read_table('wrime-ver1.tsv')\n",
    "# df_wrime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plutchik's eight primary emotions.\n",
    "# emotion_names = ['Joy', 'Sadness', 'Anticipation', 'Surprise', 'Anger', 'Fear', 'Disgust', 'Trust']\n",
    "\n",
    "# # Make a new column by making a list of mean over readersï¼ˆ\"Avg. Readers_*\"ï¼‰ \n",
    "# df_wrime['readers_emotion_intensities'] = df_wrime.apply(lambda x: [x['Avg. Readers_' + name] for name in emotion_names], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_wrime.columns)\n",
    "# print(len(df_wrime.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tweets such that the maximum intensity of the eight primary emotions is more than 2 are selected.\n",
    "# is_target = df_wrime['readers_emotion_intensities'].map(lambda x: max(x) >= 2)\n",
    "# df_wrime_target = df_wrime[is_target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Sentence  UserID  \\\n",
      "0                         ã¼ã‘ã£ã¨ã—ã¦ãŸã‚‰ã“ã‚“ãªæ™‚é–“ï½¡ãƒãƒ£ãƒªã‚ã‚‹ã‹ã‚‰é£Ÿã¹ã«ã§ãŸã„ã®ã«â€¦       1   \n",
      "1      ä»Šæ—¥ã®æœˆã‚‚ç™½ãã¦æ˜ã‚‹ã„ã€‚æ˜¨æ—¥ã‚ˆã‚Šé›²ãŒå°‘ãªãã¦ã‚­ãƒ¬ã‚¤ãª? ã¨ç«‹ã¡æ­¢ã¾ã‚‹å¸°ã‚Šé“ï½¡ãƒãƒ£ãƒªãªã—ç”Ÿæ´»ã‚‚...       1   \n",
      "2                     æ—©å¯ã™ã‚‹ã¤ã‚‚ã‚ŠãŒé£²ã¿ç‰©ãŒãªããªã‚Šã‚³ãƒ³ãƒ“ãƒ‹ã¸ï½¡ã‚“ï½¤ä»Šæ—¥ã€é¢¨ãŒæ¶¼ã—ã„ãªã€‚       1   \n",
      "3                                               çœ ã„ã€çœ ã‚Œãªã„ã€‚       1   \n",
      "4        ãŸã ã„ã¾? ã£ã¦æ–°ä½“æ“ã—ã¦ã‚‹ã‚„ã‚“!å¤–é£Ÿã™ã‚‹æ°—æº€ã€…ã§å®¶ã«ä½•ã‚‚ãªã„ã®ã«!ãƒ†ãƒ¬ãƒ“ã‹ã‚‰é›¢ã‚Œã‚‰ã‚Œãªã„â€¦!       1   \n",
      "...                                                  ...     ...   \n",
      "43195  çœŸå¤œä¸­ã«ãµã¨æ€ã„ç«‹ã¡ã€ãƒãƒ¼ãƒˆPCã‚’æŒã£ã¦éƒ¨å±‹ã‚’å‡ºã¦ã€ãƒ€ã‚¤ãƒ‹ãƒ³ã‚°ã§ä»•äº‹ã—ãŸã‚‰ã™ã‚“ã”ã„æ—ã£ãŸã€‚\\...      80   \n",
      "43196  ãã£ã©ã“ã‚“ã§ãƒã—ã‚‡ã‚“ã€‚\\nå¿ƒã‚‚é ­ã‚‚ã‚¯ãƒªã‚¢ã€‚\\nç§‹åˆ†ã®æ—¥ã®ãŠã‹ã’ã‹ãªï¼Ÿ\\näººã¨è‡ªç„¶ã¨ã—ã£ã¨ã‚Šé...      80   \n",
      "43197  æœã‹ã‚‰å…è¨±ã®æ›´æ–°ã¸ã€‚\\n90åˆ†ã§çµ‚ã‚ã‚Šã€å‡ºå£ã¸å‘ã‹ã†ã¨çŒ®è¡€ã®å‘¼ã³ã‹ã‘ãŒã€‚\\nã¿ã‚“ãªé€šã‚Šéãã¦...      80   \n",
      "43198  å¤œã‚‚æ›´ã‘ã¦å‚ã‚Šã¾ã—ãŸãŒã€é£Ÿå¾Œã®ã‚³ãƒ¼ãƒ’ãƒ¼ãŒé£²ã¿ãŸã„ã®ã§ãƒ‰ãƒªãƒƒãƒ—é–‹å§‹â€¦\\n\\nã¼ã‚“ã‚„ã‚Šç§‹ã®å¤œé•·ã‚’...      80   \n",
      "43199  ã‚³ãƒ¼ãƒ’ãƒ¼ä¼‘æ†©ï¼ˆkahavitaukoï¼‰\\n\\nã„ã¤ã‚‚ã®è±†ãªã®ã«ã™ã”ãç¾å‘³ã—ãã§ããŸ \\n\\n...      80   \n",
      "\n",
      "               Datetime Train/Dev/Test  Writer_Joy  Writer_Sadness  \\\n",
      "0      2012/07/31 23:48          train           0               1   \n",
      "1      2012/08/02 23:09          train           3               0   \n",
      "2      2012/08/05 00:50          train           1               1   \n",
      "3      2012/08/08 01:36          train           0               2   \n",
      "4      2012/08/09 22:24          train           2               1   \n",
      "...                 ...            ...         ...             ...   \n",
      "43195  2020/09/15 08:01          train           0               0   \n",
      "43196  2020/09/22 01:52          train           1               0   \n",
      "43197  2020/09/23 22:32          train           2               0   \n",
      "43198  2020/10/11 00:12          train           2               0   \n",
      "43199  2020/10/16 12:55          train           2               0   \n",
      "\n",
      "       Writer_Anticipation  Writer_Surprise  Writer_Anger  Writer_Fear  ...  \\\n",
      "0                        2                1             1            0  ...   \n",
      "1                        3                0             0            0  ...   \n",
      "2                        1                1             0            0  ...   \n",
      "3                        1                0             0            1  ...   \n",
      "4                        3                2             0            1  ...   \n",
      "...                    ...              ...           ...          ...  ...   \n",
      "43195                    1                0             0            0  ...   \n",
      "43196                    1                0             0            0  ...   \n",
      "43197                    2                1             0            0  ...   \n",
      "43198                    1                0             0            0  ...   \n",
      "43199                    1                0             0            0  ...   \n",
      "\n",
      "       Reader3_Trust  Avg. Readers_Joy  Avg. Readers_Sadness  \\\n",
      "0                  0                 0                     2   \n",
      "1                  1                 1                     0   \n",
      "2                  0                 0                     0   \n",
      "3                  0                 0                     1   \n",
      "4                  0                 1                     0   \n",
      "...              ...               ...                   ...   \n",
      "43195              0                 1                     0   \n",
      "43196              0                 2                     0   \n",
      "43197              0                 2                     0   \n",
      "43198              0                 0                     0   \n",
      "43199              2                 2                     0   \n",
      "\n",
      "       Avg. Readers_Anticipation  Avg. Readers_Surprise  Avg. Readers_Anger  \\\n",
      "0                              0                      0                   0   \n",
      "1                              0                      2                   0   \n",
      "2                              0                      1                   0   \n",
      "3                              0                      0                   0   \n",
      "4                              0                      1                   0   \n",
      "...                          ...                    ...                 ...   \n",
      "43195                          0                      1                   0   \n",
      "43196                          2                      0                   0   \n",
      "43197                          0                      0                   0   \n",
      "43198                          2                      0                   0   \n",
      "43199                          0                      0                   0   \n",
      "\n",
      "       Avg. Readers_Fear  Avg. Readers_Disgust  Avg. Readers_Trust  \\\n",
      "0                      0                     0                   0   \n",
      "1                      0                     0                   0   \n",
      "2                      0                     0                   0   \n",
      "3                      0                     1                   0   \n",
      "4                      0                     0                   0   \n",
      "...                  ...                   ...                 ...   \n",
      "43195                  0                     0                   0   \n",
      "43196                  0                     0                   0   \n",
      "43197                  0                     0                   0   \n",
      "43198                  0                     0                   0   \n",
      "43199                  0                     0                   1   \n",
      "\n",
      "       readers_emotion_intensities  \n",
      "0         [0, 2, 0, 0, 0, 0, 0, 0]  \n",
      "1         [1, 0, 0, 2, 0, 0, 0, 0]  \n",
      "2         [0, 0, 0, 1, 0, 0, 0, 0]  \n",
      "3         [0, 1, 0, 0, 0, 0, 1, 0]  \n",
      "4         [1, 0, 0, 1, 0, 0, 0, 0]  \n",
      "...                            ...  \n",
      "43195     [1, 0, 0, 1, 0, 0, 0, 0]  \n",
      "43196     [2, 0, 2, 0, 0, 0, 0, 0]  \n",
      "43197     [2, 0, 0, 0, 0, 0, 0, 0]  \n",
      "43198     [0, 0, 2, 0, 0, 0, 0, 0]  \n",
      "43199     [2, 0, 0, 0, 0, 0, 0, 1]  \n",
      "\n",
      "[43200 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_wrime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 17104\n",
      "test : 1133\n"
     ]
    }
   ],
   "source": [
    "# devide the whole dataset into train / test\n",
    "df_groups = df_wrime_target.groupby('Train/Dev/Test')\n",
    "df_train = df_groups.get_group('train')\n",
    "df_test = pd.concat([df_groups.get_group('dev'), df_groups.get_group('test')])\n",
    "print('train :', len(df_train))  # train : 17104\n",
    "print('test :', len(df_test))    # test : 1133\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install HuggingFace Transformers \n",
    "# - transformers \n",
    "# - datasets \n",
    "# cf. https://huggingface.co/docs/transformers/installation\n",
    "# ! pip install transformers datasets\n",
    "\n",
    "# Install fugashi and ipadic \n",
    "# ! pip install fugashi ipadic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/om/.miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# load a pre-trained model\n",
    "checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17104/17104 [00:03<00:00, 4814.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1133/1133 [00:00<00:00, 5342.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1.format change: pandas.DataFrame -> datasets.Dataset\n",
    "target_columns = ['Sentence', 'readers_emotion_intensities']\n",
    "train_dataset = Dataset.from_pandas(df_train[target_columns])\n",
    "test_dataset = Dataset.from_pandas(df_test[target_columns])\n",
    "\n",
    "# 2. apply Tokenizer\n",
    "def tokenize_function(batch):\n",
    "    \"\"\"Tokenizerã‚’é©ç”¨ ï¼ˆæ„Ÿæƒ…å¼·åº¦ã®æ­£è¦åŒ–ã‚‚åŒæ™‚ã«å®Ÿæ–½ã™ã‚‹ï¼‰.\"\"\"\n",
    "    tokenized_batch = tokenizer(batch['Sentence'], truncation=True, padding='max_length')\n",
    "    tokenized_batch['labels'] = [x / np.sum(x) for x in batch['readers_emotion_intensities']]  # normalized\n",
    "    return tokenized_batch\n",
    "\n",
    "train_tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11972/617072238.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mosamu-maruyama-9999\u001b[0m (\u001b[33mmaruyama-lab-design\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/osamu/Documents/GitHub/emotion/wandb/run-20240906_104222-brjmtptd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maruyama-lab-design/huggingface/runs/brjmtptd' target=\"_blank\">clean-eon-3</a></strong> to <a href='https://wandb.ai/maruyama-lab-design/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maruyama-lab-design/huggingface' target=\"_blank\">https://wandb.ai/maruyama-lab-design/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maruyama-lab-design/huggingface/runs/brjmtptd' target=\"_blank\">https://wandb.ai/maruyama-lab-design/huggingface/runs/brjmtptd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2138' max='2138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2138/2138 12:49:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.277081</td>\n",
       "      <td>0.581642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.249603</td>\n",
       "      <td>0.622242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.235353</td>\n",
       "      <td>0.664607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.224553</td>\n",
       "      <td>0.706973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.214347</td>\n",
       "      <td>0.728155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.219932</td>\n",
       "      <td>0.718447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.211464</td>\n",
       "      <td>0.748455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.208695</td>\n",
       "      <td>0.737864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.206068</td>\n",
       "      <td>0.753751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.240900</td>\n",
       "      <td>0.205352</td>\n",
       "      <td>0.763460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [142/142 11:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20462995767593384, 'eval_accuracy': 0.7616946160635482, 'eval_runtime': 712.5019, 'eval_samples_per_second': 1.59, 'eval_steps_per_second': 0.199, 'epoch': 1.0}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mevaluate())\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'metrics'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "\n",
    "# Evaluation Metrics\n",
    "# https://huggingface.co/docs/transformers/training\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    label_ids = np.argmax(labels, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=label_ids)\n",
    "\n",
    "# Training arguments\n",
    "# https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1.0,\n",
    "    evaluation_strategy=\"steps\", eval_steps=200)  # evaluate the test data at every 200 steps. \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=test_tokenized_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer.train()\n",
    "# Evaluation\n",
    "print(trainer.evaluate())\n",
    "\n",
    "### print(trainer.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_dir = os.path.join(\"output\", \"trained_model\")\n",
    "if not os.path.isdir(_dir):\n",
    "    os.makedirs(_dir)\n",
    "trainer.save_model(_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_names_jp = ['å–œã³', 'æ‚²ã—ã¿', 'æœŸå¾…', 'é©šã', 'æ€’ã‚Š', 'æã‚Œ', 'å«Œæ‚ª', 'ä¿¡é ¼'] \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns.set(font='IPAexGothic')\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import font_manager\n",
    "\n",
    "font_manager.fontManager.addfont(\"/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf\")\n",
    "matplotlib.rc('font', family=\"IPAexGothic\")\n",
    "\n",
    "\n",
    "# Softmax function\n",
    "# https://www.delftstack.com/ja/howto/numpy/numpy-softmax/\n",
    "def np_softmax(x):\n",
    "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return f_x\n",
    "\n",
    "def analyze_emotion(text, show_fig=False):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer(text, truncation=True, return_tensors=\"pt\")\n",
    "    tokens.to(model.device)\n",
    "    preds = model(**tokens)\n",
    "    prob = np_softmax(preds.logits.cpu().detach().numpy()[0])\n",
    "    out_dict = {n: p for n, p in zip(emotion_names_jp, prob)}\n",
    "\n",
    "    if show_fig:\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        df = pd.DataFrame(out_dict.items(), columns=['name', 'prob'])\n",
    "        sns.barplot(x='name', y='prob', data=df)\n",
    "        plt.title('Input sentence: ' + text, fontsize=15)\n",
    "    else:\n",
    "        print(out_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_emotion('ä»Šæ—¥ã‹ã‚‰é•·æœŸä¼‘æš‡ã ããƒ¼ãƒ¼ãƒ¼ï¼ï¼ï¼', show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_emotion('ã“ã®æ›¸é¡ã«ã¯ã‚³ãƒ¼ãƒ’ãƒ¼ã‹ã‹ã£ã¦ãªãã¦è‰¯ã‹ã£ãŸâ€¦ã€‚ä¸å¹¸ä¸­ã®å¹¸ã„ã ã€‚', show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyze_emotion('ãªã‚“ã§è‡ªåˆ†ã ã‘ã“ã‚“ãªç›®ã«é­ã†ã‚“ã â€¦â€¦', show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_emotion('å›ãªã‚‰ãã£ã¨ã‚„ã£ã¦ãã‚Œã‚‹ã¨æ€ã£ã¦ã„ãŸã‚ˆï¼', show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_emotion('ãˆã€ä»Šæ—¥ã£ã¦ä¼‘æ ¡ã ã£ãŸã®ï¼Ÿ', show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_emotion('æ˜æ—¥ã®ãƒ—ãƒ¬ã‚¼ãƒ³ã†ã¾ãã§ãã‚‹ã‹ãªãâ€¦', show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_emotion('ã‚ããƒ¼ã€ã‚¤ãƒ©ã‚¤ãƒ©ã™ã‚‹ã£ï¼ï¼', show_fig=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
