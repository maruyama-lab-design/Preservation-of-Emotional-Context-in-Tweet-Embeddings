{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wrime (/home/om/.cache/huggingface/datasets/shunk031___wrime/ver2/2.0.0/4b2571d8c51503a1134ff7edb2293a2ccb16632689d2f064b8afda72ec9e01ce)\n",
      "100%|██████████| 3/3 [00:00<00:00, 560.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'user_id', 'datetime', 'writer', 'reader1', 'reader2', 'reader3', 'avg_readers'],\n",
      "        num_rows: 30000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'user_id', 'datetime', 'writer', 'reader1', 'reader2', 'reader3', 'avg_readers'],\n",
      "        num_rows: 2500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'user_id', 'datetime', 'writer', 'reader1', 'reader2', 'reader3', 'avg_readers'],\n",
      "        num_rows: 2500\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"shunk031/wrime\", name=\"ver2\")\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>user_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>writer</th>\n",
       "      <th>reader1</th>\n",
       "      <th>reader2</th>\n",
       "      <th>reader3</th>\n",
       "      <th>avg_readers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ぼけっとしてたらこんな時間。チャリあるから食べにでたいのに…</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/7/31 23:48</td>\n",
       "      <td>{'joy': 0, 'sadness': 1, 'anticipation': 2, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 2, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 2, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 2, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 2, 'anticipation': 0, 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>今日の月も白くて明るい。昨日より雲が少なくてキレイな〜 と立ち止まる帰り道。チャリなし生活も...</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/8/2 23:09</td>\n",
       "      <td>{'joy': 3, 'sadness': 0, 'anticipation': 3, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 2, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 1, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 1, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>早寝するつもりが飲み物がなくなりコンビニへ。ん、今日、風が涼しいな。</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/8/5 0:50</td>\n",
       "      <td>{'joy': 1, 'sadness': 1, 'anticipation': 1, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>眠い、眠れない。</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/8/8 1:36</td>\n",
       "      <td>{'joy': 0, 'sadness': 2, 'anticipation': 1, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 2, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 1, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 1, 'anticipation': 0, 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ただいま〜 って新体操してるやん!外食する気満々で家に何もないのに!テレビから離れられない…!</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/8/9 22:24</td>\n",
       "      <td>{'joy': 2, 'sadness': 1, 'anticipation': 3, 's...</td>\n",
       "      <td>{'joy': 0, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 2, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 1, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "      <td>{'joy': 1, 'sadness': 0, 'anticipation': 0, 's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence user_id         datetime  \\\n",
       "0                     ぼけっとしてたらこんな時間。チャリあるから食べにでたいのに…       1  2012/7/31 23:48   \n",
       "1  今日の月も白くて明るい。昨日より雲が少なくてキレイな〜 と立ち止まる帰り道。チャリなし生活も...       1   2012/8/2 23:09   \n",
       "2                 早寝するつもりが飲み物がなくなりコンビニへ。ん、今日、風が涼しいな。       1    2012/8/5 0:50   \n",
       "3                                           眠い、眠れない。       1    2012/8/8 1:36   \n",
       "4    ただいま〜 って新体操してるやん!外食する気満々で家に何もないのに!テレビから離れられない…!       1   2012/8/9 22:24   \n",
       "\n",
       "                                              writer  \\\n",
       "0  {'joy': 0, 'sadness': 1, 'anticipation': 2, 's...   \n",
       "1  {'joy': 3, 'sadness': 0, 'anticipation': 3, 's...   \n",
       "2  {'joy': 1, 'sadness': 1, 'anticipation': 1, 's...   \n",
       "3  {'joy': 0, 'sadness': 2, 'anticipation': 1, 's...   \n",
       "4  {'joy': 2, 'sadness': 1, 'anticipation': 3, 's...   \n",
       "\n",
       "                                             reader1  \\\n",
       "0  {'joy': 0, 'sadness': 2, 'anticipation': 0, 's...   \n",
       "1  {'joy': 0, 'sadness': 0, 'anticipation': 0, 's...   \n",
       "2  {'joy': 0, 'sadness': 0, 'anticipation': 0, 's...   \n",
       "3  {'joy': 0, 'sadness': 0, 'anticipation': 0, 's...   \n",
       "4  {'joy': 0, 'sadness': 0, 'anticipation': 0, 's...   \n",
       "\n",
       "                                             reader2  \\\n",
       "0  {'joy': 0, 'sadness': 2, 'anticipation': 0, 's...   \n",
       "1  {'joy': 2, 'sadness': 0, 'anticipation': 0, 's...   \n",
       "2  {'joy': 0, 'sadness': 0, 'anticipation': 0, 's...   \n",
       "3  {'joy': 0, 'sadness': 2, 'anticipation': 0, 's...   \n",
       "4  {'joy': 2, 'sadness': 0, 'anticipation': 0, 's...   \n",
       "\n",
       "                                             reader3  \\\n",
       "0  {'joy': 0, 'sadness': 2, 'anticipation': 0, 's...   \n",
       "1  {'joy': 1, 'sadness': 0, 'anticipation': 0, 's...   \n",
       "2  {'joy': 0, 'sadness': 0, 'anticipation': 0, 's...   \n",
       "3  {'joy': 0, 'sadness': 1, 'anticipation': 0, 's...   \n",
       "4  {'joy': 1, 'sadness': 0, 'anticipation': 0, 's...   \n",
       "\n",
       "                                         avg_readers  \n",
       "0  {'joy': 0, 'sadness': 2, 'anticipation': 0, 's...  \n",
       "1  {'joy': 1, 'sadness': 0, 'anticipation': 0, 's...  \n",
       "2  {'joy': 0, 'sadness': 0, 'anticipation': 0, 's...  \n",
       "3  {'joy': 0, 'sadness': 1, 'anticipation': 0, 's...  \n",
       "4  {'joy': 1, 'sadness': 0, 'anticipation': 0, 's...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an option. \n",
    "# Currently, do not execute:\n",
    "dataset.set_format(type=\"pandas\")\n",
    "train_df = dataset[\"train\"][:]\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ぼけっとしてたらこんな時間。チャリあるから食べにでたいのに…\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "# a = train_df.loc[0,\"writer\"][\"sentiment\"]\n",
    "a = train_df.loc[0,\"sentence\"]\n",
    "print(a)\n",
    "print(type(a))\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"sentence\"], padding=True, truncation=True)\n",
    "\n",
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence                                                                                                 datetime       \n",
       " はっ！こんな時間。でも慌てて眠らなくていいんだ！コロッケ食べてやる！                                                                      2013/1/21 5:03     1\n",
       "夫がみんゴル買ったんですけど、最初面白さが全く分からなくてクソゲーくらいに思っていたんですが、段々プレイを見ているのが楽しくなってきて、やってみなよ〜とコントローラーを渡されてからハマるまではほぼ秒でしたね  2020/7/29 8:36     1\n",
       "夫の上の弟のところは直接の原因は知らないけれど、もう離婚してる。元奥さんは資格持ちなので、スパッと決断が速かった。やはり経済力は大事。                                      2020/9/18 0:26     1\n",
       "夫のスマホを代理で機種変してきた。私の別件の手続きやらも発生して2時間超かかった。お腹空いたから、チルド餃子焼いてる。                                              2020/9/1 14:43     1\n",
       "夫のスマホの細々した設定するついでにこっそりcocoaインストールしておいた。                                                                  2020/9/2 21:35     1\n",
       "                                                                                                                           ..\n",
       "わお！すっごい拡散されてる…！                                                                                          2017/3/26 19:07    1\n",
       "わいわいとした宅飲みがしたいです                                                                                         2015/7/2 22:46     1\n",
       "わいもテレワークしたいゎ                                                                                             2020/2/27 0:01     1\n",
       "わいもわいも踊りたい！                                                                                              2020/4/2 23:50     1\n",
       "ｗｗｗｗｗｗｗｗｗ                                                                                                2012/9/2 2:18      1\n",
       "Length: 30000, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.value_counts([\"sentence\", \"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'user_id': Value(dtype='string', id=None),\n",
       " 'datetime': Value(dtype='string', id=None),\n",
       " 'writer': {'joy': Value(dtype='int8', id=None),\n",
       "  'sadness': Value(dtype='int8', id=None),\n",
       "  'anticipation': Value(dtype='int8', id=None),\n",
       "  'surprise': Value(dtype='int8', id=None),\n",
       "  'anger': Value(dtype='int8', id=None),\n",
       "  'fear': Value(dtype='int8', id=None),\n",
       "  'disgust': Value(dtype='int8', id=None),\n",
       "  'trust': Value(dtype='int8', id=None),\n",
       "  'sentiment': Value(dtype='int8', id=None)},\n",
       " 'reader1': {'joy': Value(dtype='int8', id=None),\n",
       "  'sadness': Value(dtype='int8', id=None),\n",
       "  'anticipation': Value(dtype='int8', id=None),\n",
       "  'surprise': Value(dtype='int8', id=None),\n",
       "  'anger': Value(dtype='int8', id=None),\n",
       "  'fear': Value(dtype='int8', id=None),\n",
       "  'disgust': Value(dtype='int8', id=None),\n",
       "  'trust': Value(dtype='int8', id=None),\n",
       "  'sentiment': Value(dtype='int8', id=None)},\n",
       " 'reader2': {'joy': Value(dtype='int8', id=None),\n",
       "  'sadness': Value(dtype='int8', id=None),\n",
       "  'anticipation': Value(dtype='int8', id=None),\n",
       "  'surprise': Value(dtype='int8', id=None),\n",
       "  'anger': Value(dtype='int8', id=None),\n",
       "  'fear': Value(dtype='int8', id=None),\n",
       "  'disgust': Value(dtype='int8', id=None),\n",
       "  'trust': Value(dtype='int8', id=None),\n",
       "  'sentiment': Value(dtype='int8', id=None)},\n",
       " 'reader3': {'joy': Value(dtype='int8', id=None),\n",
       "  'sadness': Value(dtype='int8', id=None),\n",
       "  'anticipation': Value(dtype='int8', id=None),\n",
       "  'surprise': Value(dtype='int8', id=None),\n",
       "  'anger': Value(dtype='int8', id=None),\n",
       "  'fear': Value(dtype='int8', id=None),\n",
       "  'disgust': Value(dtype='int8', id=None),\n",
       "  'trust': Value(dtype='int8', id=None),\n",
       "  'sentiment': Value(dtype='int8', id=None)},\n",
       " 'avg_readers': {'joy': Value(dtype='int8', id=None),\n",
       "  'sadness': Value(dtype='int8', id=None),\n",
       "  'anticipation': Value(dtype='int8', id=None),\n",
       "  'surprise': Value(dtype='int8', id=None),\n",
       "  'anger': Value(dtype='int8', id=None),\n",
       "  'fear': Value(dtype='int8', id=None),\n",
       "  'disgust': Value(dtype='int8', id=None),\n",
       "  'trust': Value(dtype='int8', id=None),\n",
       "  'sentiment': Value(dtype='int8', id=None)}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.miniconda3/envs/transformers/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.miniconda3/envs/transformers/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.miniconda3/envs/transformers/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlabel_int2str\u001b[39m(x):\n\u001b[1;32m      2\u001b[0m \t\u001b[39mreturn\u001b[39;00m dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mfeatures[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mint2str(x)\n\u001b[0;32m----> 4\u001b[0m train_df[\u001b[39m\"\u001b[39m\u001b[39mlabel_name\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mapply(label_int2str)\n\u001b[1;32m      5\u001b[0m train_df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.miniconda3/envs/transformers/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.miniconda3/envs/transformers/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "def label_int2str(x):\n",
    "\treturn dataset[\"train\"].features[\"label\"].int2str(x)\n",
    "\n",
    "train_df[\"label_name\"] = train_df[\"label\"].apply(label_int2str)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.11986054480075836, 'token': 14532, 'token_str': 'チョコレート', 'sequence': '誕生 日 に チョコレート を 食べる の が 楽しみ だ 。'}, {'score': 0.07337542623281479, 'token': 17960, 'token_str': 'ケーキ', 'sequence': '誕生 日 に ケーキ を 食べる の が 楽しみ だ 。'}, {'score': 0.049764297902584076, 'token': 3469, 'token_str': 'パン', 'sequence': '誕生 日 に パン を 食べる の が 楽しみ だ 。'}, {'score': 0.04545459523797035, 'token': 5299, 'token_str': 'ワイン', 'sequence': '誕生 日 に ワイン を 食べる の が 楽しみ だ 。'}, {'score': 0.041542962193489075, 'token': 2990, 'token_str': '肉', 'sequence': '誕生 日 に 肉 を 食べる の が 楽しみ だ 。'}]\n",
      "[{'score': 0.13585378229618073, 'token': 2845, 'token_str': '生徒', 'sequence': '校庭 に 生徒 が 入っ て き て 、 皆 大騒ぎ だ 。'}, {'score': 0.06785719096660614, 'token': 53, 'token_str': '人', 'sequence': '校庭 に 人 が 入っ て き て 、 皆 大騒ぎ だ 。'}, {'score': 0.03141533583402634, 'token': 2928, 'token_str': '犬', 'sequence': '校庭 に 犬 が 入っ て き て 、 皆 大騒ぎ だ 。'}, {'score': 0.030850958079099655, 'token': 15573, 'token_str': '幽霊', 'sequence': '校庭 に 幽霊 が 入っ て き て 、 皆 大騒ぎ だ 。'}, {'score': 0.02935044839978218, 'token': 1803, 'token_str': '子供', 'sequence': '校庭 に 子供 が 入っ て き て 、 皆 大騒ぎ だ 。'}]\n",
      "[{'score': 0.06549612432718277, 'token': 3341, 'token_str': '声優', 'sequence': '将来 は 、 声優 に なる の が 夢 だ 。'}, {'score': 0.04995381459593773, 'token': 2125, 'token_str': '俳優', 'sequence': '将来 は 、 俳優 に なる の が 夢 だ 。'}, {'score': 0.046676602214574814, 'token': 3648, 'token_str': 'アナウンサー', 'sequence': '将来 は 、 アナウンサー に なる の が 夢 だ 。'}, {'score': 0.043845806270837784, 'token': 2396, 'token_str': '女優', 'sequence': '将来 は 、 女優 に なる の が 夢 だ 。'}, {'score': 0.04119384288787842, 'token': 2698, 'token_str': '歌手', 'sequence': '将来 は 、 歌手 に なる の が 夢 だ 。'}]\n",
      "[{'score': 0.8085205554962158, 'token': 1201, 'token_str': '野球', 'sequence': '太郎 は 、 野球 部 の エース だ 。'}, {'score': 0.02859513647854328, 'token': 70, 'token_str': 'この', 'sequence': '太郎 は 、 この 部 の エース だ 。'}, {'score': 0.024342620745301247, 'token': 1301, 'token_str': 'サッカー', 'sequence': '太郎 は 、 サッカー 部 の エース だ 。'}, {'score': 0.011369869112968445, 'token': 6252, 'token_str': 'ラグビー', 'sequence': '太郎 は 、 ラグビー 部 の エース だ 。'}, {'score': 0.008638503961265087, 'token': 3124, 'token_str': '陸上', 'sequence': '太郎 は 、 陸上 部 の エース だ 。'}]\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "from transformers import BertConfig, BertJapaneseTokenizer, BertForMaskedLM\n",
    "from transformers import pipeline\n",
    "\n",
    "config = BertConfig.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer, config=config)\n",
    "print(fill_mask('誕生日に[MASK]を食べるのが楽しみだ。'))\n",
    "print(fill_mask('校庭に[MASK]が入ってきて、皆大騒ぎだ。'))\n",
    "print(fill_mask('将来は、[MASK]になるのが夢だ。'))\n",
    "print(fill_mask('太郎は、[MASK]部のエースだ。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[391, 8111, 12, 1201, 11, 14847, 34, 8]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "path = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(path, word_tokenizer_type='mecab')\n",
    "tokenizer.encode(text='東京五輪で野球を観戦する。', add_special_tokens=False)\n",
    "# [391, 8111, 12, 1201, 11, 14847, 34, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 3\n",
    "\n",
    "model_ckpt = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "model = (AutoModelForSequenceClassification\n",
    "    .from_pretrained(model_ckpt, num_labels=num_labels)\n",
    "    .to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
