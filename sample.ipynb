{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/om/.miniconda3/envs/transformers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 6.23k/6.23k [00:00<00:00, 13.0MB/s]\n",
      "Downloading metadata: 100%|██████████| 26.2k/26.2k [00:00<00:00, 49.1MB/s]\n",
      "Downloading readme: 100%|██████████| 1.21k/1.21k [00:00<00:00, 5.10MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset multilingual-sentiments/japanese (download: 38.00 MiB, generated: 38.64 MiB, post-processed: Unknown size, total: 76.63 MiB) to /home/om/.cache/huggingface/datasets/tyqiangz___multilingual-sentiments/japanese/1.0.0/b7cdd8874d82679e59432edf79e074f595c4ad26d2e562eba4fb55f361691b07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 38.0MB [00:00, 147MB/s]                    \n",
      "Downloading data: 929kB [00:00, 59.5MB/s]                   \n",
      "Downloading data: 941kB [00:00, 47.9MB/s]                   \n",
      "Generating train split:   2%|▏         | 2130/120000 [00:00<00:21, 5359.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "                                                text     label  \\\n",
      "0  普段使いとバイクに乗るときのブーツ兼用として購入しました。見た目や履き心地は良いです。 しか...  negative   \n",
      "1  十分な在庫を用意できない販売元も悪いですが、Amazonやら楽⚪︎が転売を認めちゃってるのが...  negative   \n",
      "2  見た目はかなりおしゃれで気に入りました。2、3回持ち歩いた後いつも通りゼンマイを巻いていたら...  negative   \n",
      "3  よくある部分での断線はしませんでした ただiphoneとの接続部で接触不良、折れました ip...  negative   \n",
      "4  プラモデルの塗装剥離に使う為に購入 届いて早速使ってみた 結果 １ヶ月経っても未だに剥離出来...  negative   \n",
      "\n",
      "                 source  \n",
      "0  amazon_reviews_multi  \n",
      "1  amazon_reviews_multi  \n",
      "2  amazon_reviews_multi  \n",
      "3  amazon_reviews_multi  \n",
      "4  amazon_reviews_multi  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "                                                text     label  \\\n",
      "0  味自体及び吸い心地は良いのだが、不良品が多過ぎる。私の場合５本のうち２本が蒸気も出ず、吸い込...  negative   \n",
      "1             ホームボタン周りの気泡が全く抜けません。 返金をお願いしましたが、断られた。  negative   \n",
      "2  新旧含めて4つのカーテンレールがあるのですが、使用出来るカーテンレールはありませんでした。 ...  negative   \n",
      "3            予約注文でしたが、どこから特典であるpdfダウンロードすればよいのでしょうか…  negative   \n",
      "4  前のレビューにもありましたが、片方が全く動きません。 返品しようにも、なんだかめんどくさいし...  negative   \n",
      "\n",
      "                 source  \n",
      "0  amazon_reviews_multi  \n",
      "1  amazon_reviews_multi  \n",
      "2  amazon_reviews_multi  \n",
      "3  amazon_reviews_multi  \n",
      "4  amazon_reviews_multi  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "                                                text     label  \\\n",
      "0  購入、貼付け後2週間もたたないうちに、上側から剥がれてきて画面から浮いた状態になってしまった...  negative   \n",
      "1  以下の3点の理由により、期待はずれの粗悪品。 ①他の方のレビューにもある通り、天板の外観が掲...  negative   \n",
      "2  この商品の内容等確認した購入したのですが、そのとおりなかなか設定ができなく、知人にもお願いし...  negative   \n",
      "3  テストした結果、4000mahのスマホ一回と30％分、スマホのバッテリー残量の表示が正しけれ...  negative   \n",
      "4  前回の黒いドレッサーバッグの評判がよかったため、予約して購入しました。 ガッカリです。 ヨレ...  negative   \n",
      "\n",
      "                 source  \n",
      "0  amazon_reviews_multi  \n",
      "1  amazon_reviews_multi  \n",
      "2  amazon_reviews_multi  \n",
      "3  amazon_reviews_multi  \n",
      "4  amazon_reviews_multi  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset multilingual-sentiments downloaded and prepared to /home/om/.cache/huggingface/datasets/tyqiangz___multilingual-sentiments/japanese/1.0.0/b7cdd8874d82679e59432edf79e074f595c4ad26d2e562eba4fb55f361691b07. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 671.70it/s]\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/om/.miniconda3/envs/transformers/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# データセット取得\n",
    "dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"japanese\")\n",
    "\n",
    "# トークナイザの取得\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "# モデルの取得\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = dataset[\"train\"].features[\"label\"].num_classes\n",
    "model = (AutoModelForSequenceClassification\n",
    "    .from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", num_labels=num_labels)\n",
    "    .to(device))\n",
    "\n",
    "# トークナイザ処理\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "\n",
    "# トレーニング準備\n",
    "batch_size = 16\n",
    "logging_steps = len(dataset_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"sample-text-classification-distilbert\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False,\n",
    "    log_level=\"error\",\n",
    ")\n",
    "\n",
    "# 評価指標の定義\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# トレーニング\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 評価\n",
    "preds_output = trainer.predict(dataset_encoded[\"validation\"])\n",
    "\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_valid = np.array(dataset_encoded[\"validation\"][\"label\"])\n",
    "labels = dataset_encoded[\"train\"].features[\"label\"].names\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)\n",
    "\n",
    "# ラベル情報付与\n",
    "id2label = {}\n",
    "for i in range(dataset[\"train\"].features[\"label\"].num_classes):\n",
    "    id2label[i] = dataset[\"train\"].features[\"label\"].int2str(i)\n",
    "\n",
    "label2id = {}\n",
    "for i in range(dataset[\"train\"].features[\"label\"].num_classes):\n",
    "    label2id[dataset[\"train\"].features[\"label\"].int2str(i)] = i\n",
    "\n",
    "trainer.model.config.id2label = id2label\n",
    "trainer.model.config.label2id = label2id\n",
    "\n",
    "# 保存\n",
    "trainer.save_model(f\"/content/drive/MyDrive/sample-text-classification-bert\")\n",
    "\n",
    "# ロード\n",
    "new_tokenizer = AutoTokenizer\\\n",
    "    .from_pretrained(f\"/content/drive/MyDrive/sample-text-classification-bert\")\n",
    "\n",
    "new_model = (AutoModelForSequenceClassification\n",
    "    .from_pretrained(f\"/content/drive/MyDrive/sample-text-classification-bert\")\n",
    "    .to(device))\n",
    "\n",
    "# サンプルテキストで推論\n",
    "inputs = new_tokenizer(sample_text, return_tensors=\"pt\")\n",
    "new_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = new_model(\n",
    "        inputs[\"input_ids\"].to(device), \n",
    "        inputs[\"attention_mask\"].to(device),\n",
    "    )\n",
    "y_preds = np.argmax(outputs.logits.to('cpu').detach().numpy().copy(), axis=1)\n",
    "def id2label(x):\n",
    "    return new_model.config.id2label[x]\n",
    "y_dash = [id2label(x) for x in y_preds]\n",
    "y_dash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
